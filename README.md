
# WIFI-TER-SIM — Reproducible ns-3 Wi-Fi Experiment Suite (Parts 1–8)

## 1. Purpose and Scientific Scope

**WIFI-TER-SIM** is a structured ns-3 simulation suite designed as an academic / research-oriented workflow for Wi-Fi experimentation.  
The project is organized into **eight parts (Part 1 to Part 8)**. Each part consists of:

- a **C++ ns-3 scenario** (`scenarios/pX_*.cc`)
- a **Bash runner script** (`scripts/run_pX.sh`) implementing the experimental protocol
- a **Python plotting script** (`scripts/plot_pX.py`) to visualize outputs using:
```bash
  python3 scripts/plot_pX.py
````

The principal objective is **reproducibility** and **traceability**:

* deterministic control of **RNG seed/run**
* standardized execution steps: **clean → stage → build → run → merge**
* safe handling of outputs under **parallel execution**
* consistent output foldering under `results/pX/`

---

## 2. Repository Structure

```
WIFI-TER-SIM
├── docs
├── results
│   ├── p1 ... p8
├── scenarios
│   ├── p1_minimal_wifi.cc
│   ├── p2_baseline.cc
│   ├── p3_distance_sweep.cc
│   ├── p4_phy_mac_sweep.cc
│   ├── p5_multi_sta_fairness.cc
│   ├── p6_roaming_multi_ap.cc
│   ├── p7_channel_planning.cc
│   └── p8_final_simulator.cc
├── scripts
│   ├── plot_p2.py ... plot_p8.py
│   ├── run_p1.sh  ... run_p8.sh
└── .gitignore
```

### 2.1 Results Folder Convention

Each part writes its generated artifacts under `results/pX/` using a stable scientific layout:

* `results/pX/raw/`
  Primary outputs used for analysis (CSV summaries, time series CSV, FlowMonitor XML, PCAP, event traces, etc.)

* `results/pX/logs/`
  Build and runtime evidence:

  * `build.log` (compilation)
  * per-run logs (`<tag>.log`)

* `results/pX/plots/`
  Output figures generated by the plotting scripts (recommended location)

* `results/pX/tmp_runs/` (when enabled by runners)
  Temporary per-run isolated outputs used to guarantee correctness under parallelism.

**Scientific rationale:**
`raw/` stores “primary measurements”, `plots/` stores “derived interpretation”, and `logs/` acts as a laboratory notebook.

---

## 3. Requirements

### 3.1 System Requirements

* Linux/macOS strongly recommended (Bash tooling assumed)
* Standard Unix tools:

  * `find`, `tail`, `awk`, `wc`, `sort`, `cp`, `mkdir`, `rm`
* Optional but useful:

  * `rsync` (fast archiving)
  * `cpulimit`, `nice`, `ionice` (thermal / priority control)

### 3.2 ns-3 Requirement

* ns-3 installed locally. Runner scripts assume by default:

  * `NS3_DIR=$HOME/ns-3`

Override example:

```bash
NS3_DIR=/path/to/ns-3 ./scripts/run_p4.sh
```

### 3.3 Python Requirement (Plotting)

* Python 3 for all `plot_pX.py` scripts:

```bash
python3 --version
```

---

## 4. Scientific Execution Protocol (Runner Scripts)

All `run_pX.sh` scripts follow a consistent experimental protocol:

### Step A — Clean (prevents dataset contamination)

Most runners begin by removing old outputs:

* ensures results correspond to the current run
* prevents accidental mixing of previous runs

Typical behavior:

```bash
rm -rf results/pX/raw results/pX/logs results/pX/plots results/pX/tmp_runs
mkdir -p results/pX/raw results/pX/logs results/pX/plots results/pX/tmp_runs
```

### Step B — Stage Scenario into ns-3 scratch

Because ns-3 commonly runs programs under `scratch/`, runners copy scenario sources:

* `scenarios/pX_*.cc → $NS3_DIR/scratch/<scenario>.cc`

This prevents running an outdated scratch file.

### Step C — Build ns-3 (with logging)

Runners compile once and store build logs:

* `results/pX/logs/build.log`

### Step D — Execute Run Matrix (Sequential or Parallel)

Runners define a parameter sweep (grid) using environment variables, e.g.:

* distances, speeds, MCS, rate modes, number of STAs, QoS modes, etc.

### Step E — Merge Outputs

When runs are executed in isolated directories, runners merge:

* per-run CSV into consolidated CSV under `results/pX/raw/`
* per-run time series into unified datasets with run identifiers
* FlowMonitor XML files collected per run

### Step F — Cleanup / Archive

Some runners archive per-run artifacts under:

* `results/pX/raw/per_run_raw/`
  and optionally remove `tmp_runs/`.

---

## 5. Parallelization (Correctness-Safe Concurrency)

Many parts use parallel execution to accelerate sweeps.
The scientific rule is:

> **Parallelization is valid only if each run writes to its own private output directory.**

Therefore, a run typically uses:

* `--outDir=results/pX/tmp_runs/<tag>`

This prevents:

* output overwrites (e.g., `flowmon.xml`)
* CSV corruption from concurrent appends
* nondeterministic or mixed datasets

### 5.1 Bounded Concurrency (JOBS)

Parallel scripts limit concurrency using `JOBS`:

```bash
JOBS=6 ./scripts/run_p6.sh
```

### 5.2 Progress Counter Requirement `[i/total]`

All runners are expected to print progress counters such as:

* `[1/6] RUN start ...`
* `[1/6] RUN ok ...`

This provides:

* observability for long sweeps
* rapid identification of failing configurations
* clarity even when completion order differs (parallel scheduling)

---

## 6. Reproducibility Controls (Seed/Run)

### 6.1 Seed and Run

Most parts expose:

* `SEED` : RNG seed
* `RUN_BASE` : starting run id

Example:

```bash
SEED=1 RUN_BASE=1 ./scripts/run_p5.sh
```

**Scientific note:**
In ns-3, reproducibility is usually guaranteed when both `Seed` and `Run` are fixed, and the scenario code + parameters remain unchanged.

### 6.2 Tags (Run Identity)

Runs typically generate a tag encoding parameters, seed, and run:

* `p6_v1.0_run3`
* `p8_ON_be40_s1_r3`

Tags are used for:

* output folder name
* per-run log file name
* traceable identifiers for post-analysis

---

## 7. Plotting and Visualization (Python)

Each part provides a Python plotting script under `scripts/`:

* `plot_p2.py, plot_p3.py, ..., plot_p8.py`

Standard invocation:

```bash
python3 scripts/plot_pX.py
```

Recommended workflow:

1. run simulations:

```bash
./scripts/run_pX.sh
```

2. generate plots:

```bash
python3 scripts/plot_pX.py
```

Plots should be written into:

* `results/pX/plots/`

---

## 8. Parts Overview (1–8)

This section documents each part as a unit of experimentation.

### Part 1 — Minimal Wi-Fi Connectivity (Sanity Baseline)

* Scenario: `scenarios/p1_minimal_wifi.cc`
* Runner: `scripts/run_p1.sh`
* Purpose: validate basic AP–STA association and connectivity.
* Typical outputs:

  * PCAP traces (if enabled)
  * a small summary text/log file

Run:

```bash
./scripts/run_p1.sh
python3 scripts/plot_p1.py   # (if present; otherwise Part 1 may not require plotting)
```

### Part 2 — Baseline Throughput + RTT Time Series

* Scenario: `scenarios/p2_baseline.cc`
* Runner: `scripts/run_p2.sh`
* Plot: `scripts/plot_p2.py`
* Outputs (typical):

  * `results/p2/raw/p2_summary.csv`
  * `results/p2/raw/throughput_timeseries.csv`
  * `results/p2/raw/rtt_timeseries.csv`
  * `results/p2/raw/flowmon.xml` (or per-run naming if runner isolates)

### Part 3 — Distance Sweep (Channel/Propagation Sensitivity)

* Scenario: `scenarios/p3_distance_sweep.cc`
* Runner: `scripts/run_p3.sh`
* Plot: `scripts/plot_p3.py`
* Scientific objective: quantify how throughput and RTT evolve vs distance.
* Notes:

  * may generate `p3_sweep.csv`
  * may generate per-distance time series and flowmon outputs

### Part 4 — PHY/MAC Sweep (Large Matrix)

* Scenario: `scenarios/p4_phy_mac_sweep.cc`
* Runner: `scripts/run_p4.sh`
* Plot: `scripts/plot_p4.py`
* Scientific objective: explore multi-dimensional configuration space (distance, width, tx power, rate mode, MCS, etc.)
* Characteristics:

  * often uses throttling / load control due to many runs
  * produces a consolidated matrix CSV for analysis

### Part 5 — Multi-STA Fairness (UDP/TCP)

* Scenario: `scenarios/p5_multi_sta_fairness.cc`
* Runner: `scripts/run_p5.sh`
* Plot: `scripts/plot_p5.py`
* Scientific objective:

  * evaluate fairness vs number of STAs and transport type.
* Typical consolidated outputs:

  * `p5_summary.csv`
  * per-STA metrics
  * time series metrics

### Part 6 — Roaming with Multiple APs Under Traffic

* Scenario: `scenarios/p6_roaming_multi_ap.cc`
* Runner: `scripts/run_p6.sh`
* Plot: `scripts/plot_p6.py`
* Scientific objective:

  * quantify throughput disruption and event timeline during roaming.
* Typical consolidated outputs:

  * summary CSV
  * throughput time series merged across runs
  * roaming events trace merged across runs

### Part 7 — Channel Planning / Spatial Study

* Scenario: `scenarios/p7_channel_planning.cc`
* Runner: `scripts/run_p7.sh`
* Plot: `scripts/plot_p7.py`
* Scientific objective:

  * evaluate channel assignment / planning / spatial performance maps.

### Part 8 — Integrated Final Simulator (QoS + Heatmaps)

* Scenario: `scenarios/p8_final_simulator.cc`
* Runner: `scripts/run_p8.sh`
* Plot: `scripts/plot_p8.py`

Observed structure under `results/p8/`:

* `heatmaps/heatmap.csv` (grid data for heatmap visualization)
* `raw/` includes:

  * `baseline_summary.csv`
  * `qos_summary.csv`
  * `p8_summary.csv`
  * many per-run FlowMonitor XMLs with naming convention:

    * `flowmon_heat_x<val>_y<val>_runN.xml`
    * `flowmon_qos_on_runN.xml`, `flowmon_qos_off_runN.xml`
* `logs/`, `plots/`

This part typically combines:

* baseline evaluation
* QoS ON/OFF comparison
* spatial grid sweeps for heatmap generation

---

## 9. Example End-to-End Usage

### 9.1 Run one part

```bash
NS3_DIR=~/ns-3 JOBS=6 ./scripts/run_p6.sh
python3 scripts/plot_p6.py
```

### 9.2 Run all parts (manual sequence)

```bash
./scripts/run_p1.sh
./scripts/run_p2.sh
./scripts/run_p3.sh
./scripts/run_p4.sh
./scripts/run_p5.sh
./scripts/run_p6.sh
./scripts/run_p7.sh
./scripts/run_p8.sh

python3 scripts/plot_p2.py
python3 scripts/plot_p3.py
python3 scripts/plot_p4.py
python3 scripts/plot_p5.py
python3 scripts/plot_p6.py
python3 scripts/plot_p7.py
python3 scripts/plot_p8.py
```

---

## 10. Troubleshooting (Operational + Scientific)

### Build failures

* Check:

  * `results/pX/logs/build.log`

### Run failures

* Check:

  * `results/pX/logs/<tag>.log`

### Common root causes

* Incorrect `NS3_DIR`
* Scenario not staged into scratch
* Output collisions due to missing per-run isolation (especially in parallel)
* Too many parallel runs causing OS thrashing / thermal throttling
  → reduce `JOBS`

---

## 11. Scientific Reporting Notes (What to Include in Your Report)

For scientific transparency, each experiment section should report:

* Part number and scenario name
* ns-3 version + environment assumptions
* parameter sweep definition (ranges)
* seed/run conventions
* output datasets (CSV names) used for plots
* whether parallelization was used and the job limit (`JOBS`)
* how consolidated datasets were produced (merge method)

---

## 12. License / Academic Use

This repository is intended for academic simulation workflows emphasizing reproducibility and traceability.
The runner script methodology is designed to be reportable as a formal experimental protocol.

---

```
